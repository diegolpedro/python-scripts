{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementacion de Grid Search, Pipelines, NLP con corpus de Twitter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intro\n",
    "Analisis de sentimiento sobre mensajes de twitter mediante la aplicacion de tecnicas de NLP (Natural Language Processing) y posteriormente ML (Machine Learning). \n",
    "Dentro de lo que es el pre-procesamiento de los datos, se limpian y tokenizan para poder generar un BOW (Bag of Words) que permita alimentar los distintos modelos. Cada modelo es entrenado por separado y optimizado mediante la implementacion de GridSearch.\n",
    "\n",
    "#### Todo\n",
    "Implementar \"Stemming\". No se implementa en esta version por no haberse encontrado un metodo actual para corpus en idioma espaÃ±ol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import csv\n",
    "import unicodedata\n",
    "import warnings\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Clasificadores\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n",
    "# Filenames\n",
    "file_name=\"union_salida_clasificada_negpos.csv\"\n",
    "stop_file=\"custom_stopwords.txt\"    # Nombre del archivo de stopwords.\n",
    "\n",
    "\n",
    "# Regex para tokenizar correctamente.\n",
    "regex_str = [\n",
    "    r'(?:[\\w_]+)',                                        # Otras palabras\n",
    "    r'(?:\\S)'                                             # Cualquier otra cosa\n",
    "]\n",
    "# Se arman objetos para regular expresions.\n",
    "tokens_re   = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "\n",
    "# Se carga archivos de STOPWORDS\n",
    "with open(stop_file, newline='') as file:\n",
    "    stopwords = file.read().splitlines()\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "    \n",
    "# Se abre archivo con tweets y se lo recorre    \n",
    "with open(file_name, newline='') as csvfile:\n",
    "\n",
    "    reader = csv.reader(csvfile, delimiter=',', quotechar='\\\"')\n",
    "    header = next(reader)\n",
    "    \n",
    "    for row in reader:\n",
    "\n",
    "        y.append(row[0].lower())\n",
    "        \n",
    "        tweet = row[1].lower()                    # Se normaliza texto, todo a minusculas.\n",
    "        tweet = re.sub(r'@[a-z0-9_]+', '', tweet) # Se quitan menciones. @xxxxxxxx\n",
    "        tweet = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', '', tweet)\n",
    "        tweet = tweet.translate(str.maketrans('','', '.~Â¡!-_â€”#$%Â¿?:+-/Â°);(/\",*â€œâ€â€˜â€™'))\n",
    "        \n",
    "        # Manejo de emoticones.\n",
    "        emoticones = [\n",
    "                      ['ğŸ˜†',' risa'],['ğŸ˜‚','risa'],['ğŸ˜±',' asombro'],['ğŸ¥³',' felicidad'],['ğŸ’™',' amor'],['ğŸ˜',' amor'],\n",
    "                      ['ğŸ˜€',' sonreir'],['ğŸ‘',' ok'],['ğŸ¤”',' dudar'],['ğŸŠ',' alegria'],['ğŸ™',' ojala'],['ğŸ’ªğŸ»',' fuerza'],\n",
    "                      ['ğŸ˜¡',' enojo'],['ğŸ˜›',' broma'],['ğŸ˜®',' asombro'],['ğŸ¤®',' desagradable'],['ğŸ‘ğŸ»',' aplauso'],\n",
    "                      ['ğŸ˜',' canchero'],['ğŸ˜©',' decepcion'],['ğŸ˜³',' verguenza'],['ğŸ˜Š',' contento'],['ğŸ˜¥',' triste'],\n",
    "                      ['ğŸ˜¤',' furioso'],['ğŸ–•',' enojo'],['ğŸ‘',' aplauso'],['ğŸ’ª',' fuerza'],['ğŸ¤¦â€','increible'],\n",
    "                      ['ğŸ™„','duda']\n",
    "                     ]\n",
    "        for emoji in emoticones:\n",
    "            tweet = tweet.replace(emoji[0], emoji[1])\n",
    "\n",
    "        # Manejo de acentos.\n",
    "        dict_acentos = [['Ã¡','a'],['Ã©','e'],['Ã­','i'],['Ã³','o'],['Ãº','u']]\n",
    "        for acento in dict_acentos:\n",
    "            tweet = tweet.replace(acento[0], acento[1])\n",
    "            \n",
    "        # Remueve letras repetidas y deja una sola.\n",
    "        for letra in ['a','e','i','o','u','s','c']:\n",
    "            pattern = letra + '{2,}'\n",
    "            tweet = re.sub(pattern, letra, tweet)\n",
    "        \n",
    "        tweet = tweet.translate(str.maketrans('','', 'ğŸ¥ğŸ§ğŸ³ğŸ–ğŸ›«ğŸ˜‘âœˆğŸ‡¦ğŸ‡·ğŸ‡µğŸ‡¾ğŸ‘‡ğŸ™ƒâ–¶ğŸ’»â–ºâ†’â¬‡ï¸ğŸ˜’ğŸ”«ğŸ”ğŸ”¥ğŸ’€ğŸš«ğŸ˜â™‚â¤â¤â¤ğŸ˜ğŸ‘Šï£¿ğŸ¤ğŸ»'))\n",
    "        tweet = re.sub(r'\\d+', '', tweet)         # Se quitan numeros.\n",
    "        tweet = tweet.strip()\n",
    "    \n",
    "        # Tokenizado\n",
    "        tokens = tokens_re.findall(tweet)\n",
    "\n",
    "        # Remocion de stopwords\n",
    "        tokens = [token for token in tokens if token not in stopwords]\n",
    "\n",
    "        s = ' '\n",
    "        X.append(s.join(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comenzando Gridsearch!\n",
      "Neural Network\n",
      "precision 0.7977188723063486\n",
      "Parametros: {'clf__alpha': 0.1, 'clf__hidden_layer_sizes': 5, 'clf__max_iter': 300, 'clf__random_state': 8, 'clf__solver': 'lbfgs', 'tfidf__use_idf': True, 'vectorizer__ngram_range': (2, 2)}\n",
      "recall 0.7052631578947368\n",
      "Parametros: {'clf__alpha': 0.1, 'clf__hidden_layer_sizes': 5, 'clf__max_iter': 300, 'clf__random_state': 8, 'clf__solver': 'lbfgs', 'tfidf__use_idf': True, 'vectorizer__ngram_range': (2, 2)}\n",
      "SVM\n",
      "precision 0.8494813263672851\n",
      "Parametros: {'clf__C': 1, 'clf__gamma': 0.001, 'clf__kernel': 'linear', 'tfidf__use_idf': True, 'vectorizer__ngram_range': (1, 1)}\n",
      "recall 0.8315789473684211\n",
      "Parametros: {'clf__C': 1, 'clf__gamma': 0.001, 'clf__kernel': 'linear', 'tfidf__use_idf': True, 'vectorizer__ngram_range': (1, 1)}\n",
      "Naive Bayes\n",
      "precision 0.8507318968639478\n",
      "Parametros: {'clf__alpha': 1.1, 'clf__fit_prior': False, 'tfidf__use_idf': True, 'vectorizer__ngram_range': (1, 1)}\n",
      "recall 0.8447368421052631\n",
      "Parametros: {'clf__alpha': 1.1, 'clf__fit_prior': False, 'tfidf__use_idf': True, 'vectorizer__ngram_range': (1, 1)}\n",
      "Random Forest\n",
      "precision 0.7670832960538844\n",
      "Parametros: {'clf__criterion': 'entropy', 'clf__max_depth': 2, 'clf__min_samples_leaf': 1, 'tfidf__use_idf': False, 'vectorizer__ngram_range': (1, 2)}\n",
      "recall 0.7394736842105263\n",
      "Parametros: {'clf__criterion': 'gini', 'clf__max_depth': 5, 'clf__min_samples_leaf': 10, 'tfidf__use_idf': True, 'vectorizer__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "# --- Pipelines ---\n",
    "pipeline1 = Pipeline([('vectorizer', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', SVC())\n",
    "                     ])\n",
    "pipeline2 = Pipeline([('vectorizer', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', DecisionTreeClassifier())\n",
    "                     ])\n",
    "pipeline3 = Pipeline([('vectorizer', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', MLPClassifier())\n",
    "                     ])\n",
    "pipeline4 = Pipeline([('vectorizer', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', MultinomialNB())\n",
    "                     ])\n",
    "\n",
    "\n",
    "# --- Parameters ---\n",
    "param_svm = [{ 'vectorizer__ngram_range': [(1, 1),(1, 2),(2, 2)],\n",
    "               'tfidf__use_idf': (True, False),\n",
    "               'clf__kernel': ['rbf','linear'],\n",
    "               'clf__gamma': [1e-3, 1e-4, 1e-5],\n",
    "               'clf__C': [1, 10, 100, 1000]\n",
    "             }]\n",
    "param_tree = [{ 'vectorizer__ngram_range': [(1, 1),(1, 2),(2, 2)],\n",
    "                'tfidf__use_idf': (True, False),\n",
    "                'clf__criterion': ['gini', 'entropy'],\n",
    "                'clf__max_depth': [ 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15],\n",
    "                'clf__min_samples_leaf': [1, 5, 10, 20]\n",
    "              }]    \n",
    "param_red =  [{ 'vectorizer__ngram_range': [(2, 2)],\n",
    "                'tfidf__use_idf': (True, False),\n",
    "                'clf__solver': ['lbfgs'], \n",
    "                'clf__max_iter': [300], \n",
    "                'clf__alpha': 10.0 ** -np.arange(1, 7), \n",
    "                'clf__hidden_layer_sizes':np.arange(5, 10), \n",
    "                'clf__random_state':[8] \n",
    "              }]\n",
    "param_nb  =  [{ 'vectorizer__ngram_range': [(1, 1),(1, 2),(2, 2)],\n",
    "                'tfidf__use_idf': (True, False),\n",
    "                'clf__alpha': np.linspace(0.5, 1.5, 6),\n",
    "                'clf__fit_prior': [True, False] \n",
    "              }]\n",
    "\n",
    "\n",
    "# --- Scores ---\n",
    "scores = ['precision', 'recall']\n",
    "model = [\"Neural Network\", \"SVM\", \"Naive Bayes\", \"Random Forest\"]\n",
    "pips = [pipeline3, pipeline1, pipeline4, pipeline2]\n",
    "pars = [param_red, param_svm,  param_nb, param_tree]\n",
    "\n",
    "\n",
    "# --- Cros Validate ---\n",
    "cvNum = 10\n",
    "\n",
    "print(\"Comenzando Gridsearch!\")\n",
    "\n",
    "for i in range(len(pars)):\n",
    "    print(model[i])\n",
    "    for score in scores:\n",
    "        \n",
    "        gs_clf_svm = GridSearchCV( pips[i], pars[i], cv=cvNum, \n",
    "                                   scoring='%s_weighted' % score, n_jobs=-1, \n",
    "                                   verbose=0, refit=False)\n",
    "        gs_clf_svm = gs_clf_svm.fit(X, y)\n",
    "        \n",
    "        print(score, gs_clf_svm.best_score_)\n",
    "        print(\"Parametros: %s\" % gs_clf_svm.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
